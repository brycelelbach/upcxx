** Minimal documentation for the parallel Poisson solver. **
** Last modified August 31, 1999 **

This program implements a domain decomposition method for the solution
of the Poisson equation in an infinite (unbounded domain).  The
solution is found on the same domain as the domain input for the
charge (right hand side).

The most basic command line options are:
-L <length>      : The number of "cells" on a side (since this is a
		   nodal point scheme, the number of points on a side
		   is length+1).  Length is defined this way for
		   historical reasons and because length is ideally a
		   power of two this way (easier to recognize than 2^N
		   + 1).  Also, the grid spacing h = 1/length this
		   way. 

-Np <patches>	 : The number of patches in each direction.  Thus the
		   whole problem is decomposed into patches x patches
		   subdomains.  

-Nr <refinement> : The refinement ratio between fine and coarse
		   patches, also in each direction.
		  

In most cases, I set Nprocs equal to the maximum allowed, i.e. equal
to the total number of patches (thus Nprocs = patches^2).  (How to set
the number of processors for a Titanium program is implementation
dependent: if you are using something like glurun, mpprun, maybe
mpirun, then there should be an option for setting the number of
processors.  Otherwise, the environment variable TI_NODES should set
the number of processors used.)

There are two other things that you should know to get good
performance.  

First, it is necessary to have refinement significantly larger than
patches.  (If r is not large enough, there is more communication and
more non-parallel computation: refinement = 4*patches is sufficient to
avoid significant overhead.  To be more precise, the non-parallel
computation overhead should be less than 7% for refinement >=
4patches.)  

Second, it is best to have refinement*patches <= length/16.  (This
condition assures that the solves on the patches are not overly
wasteful: the algorithm requires a fine grid solution for each patch
which is larger than (length/patches)^2.  If refinement*patches is too
small, the size of the fine grid solutions can be up to
25(length/patches)^2, which is a _very_ noticeable performance hit.)

The algorithm is designed to have refinement and patches proportional
to sqrt(length) to assure a solution which is accurate to O(h^2).
This means that it is difficult to design linearly scaled speed-up
tests.  (The reasoning behind the algorithm design is that as larger
parallel computers are developed, they have both more memory per
processor and more total processors; and as larger machines become
available, we want to solve larger problems, not just the same problem
faster.)  This restriction is only affects the accuracy of the
computed solution, though, so if you are only interested in
performance benchmarks, you can freely ignore it.

For convenience, the following selections make reasonable test
problems of various sizes and parallelism:

<program_name> -L 128 -Np 1 -Nr 4
<program_name> -L 256 -Np 2 -Nr 8
<program_name> -L 512 -Np 4 -Nr 16

<program_name> -L 256 -Np 1 -Nr 4
<program_name> -L 512 -Np 2 -Nr 8
<program_name> -L 1024 -Np 4 -Nr 16

<program_name> -L 64 -Np 1 -Nr 4
<program_name> -L 256 -Np 2 -Nr 8
<program_name> -L 1024 -Np 4 -Nr 16

The first two series should require constant wall-clock time.  The
last series should show linear increase in the time required: T = 4
Nprocs, because the total problem size quadruples (in each dimension)
as the number of processors is doubled (in each dimension).  In fact,
there is significant overhead for small problems, so the time required
is actually less than predicted as the problem size is increased.
This artifact should go away as the loop overhead for foreach loops on
small arrays is reduced (hopefully soon).


The time reported by the program is the wall clock time taken by the
solver itself: the timer is started right before the solver is called
and stopped immediately after the timer exits.

For the standard test case, the program reports the errors in the
calculated solution.  This requires that the program also calculate
the "exact" solution.  The time taken to calculate the exact solution
or to compare the two solutions is not included in the reported time.

This code has a solver which is designed to run well in parallel, but
several of the other parts of the code aren't parallelized or
optimized.  With that said, if you're interested mainly in timing the
performance of the parallel solver and minimizing the time required
for the entire run, it would probably be best to comment out a lot of
the trailing stuff.  This can all be done at once by commenting out
the line

myProblem.wrapUp();

in pmg_test.ti.

Most file i/o is (or should be) turned off in almost all cases.  Even
if it were parallelized (which it isn't yet), writing to files is
painfully slow from titanium and across the cs nfs system.  Truly
painful.

If you have any further questions let me know.  I would like to keep
this document current and accurate.

-Greg

gballs@cs.berkeley.edu